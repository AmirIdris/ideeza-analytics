This repository contains the solution for the Senior Backend Developer assessment. It implements a high-performance Analytics API using Django, PostgreSQL, and Redis.

## ‚ö†Ô∏è Important Update: Schema Refactor

**If you have run this project previously, you MUST reset your database.**
I have normalized the database schema (migrating `Country` from a string to a relational model) based on code review feedback.

**Run this to reset:**
```bash
docker-compose down -v  # Deletes the old volume
docker-compose exec web python manage.py migrate
docker-compose exec web python manage.py seed_data
```

## üöÄ Quick Start

The project is fully containerized. You only need Docker installed.

### 1. Start the Infrastructure
```bash
make up
# Or: docker-compose up -d --build
```

### 2. Apply Migrations
```bash
make migrate
```

### 3. üå± Seed Mock Data (Crucial)
This command generates Users, Blogs, and **10,000+ View records** distributed over the last year to simulate a production environment. **The APIs will return empty lists without this step.**
```bash
make seed
```

4. **Access Documentation:**
   * **Swagger UI:** [http://localhost:8000/swagger/](http://localhost:8000/swagger/)
   * **API Root:** [http://localhost:8000/api/analytics/](http://localhost:8000/api/analytics/)

---

---

## üì° API Reference & Testing

All endpoints accept a **POST** request with a filter payload. The filters use **Explicit Typing** to ensure security and readability.

### Filter Parameters Table
| Parameter | Type | Description |
| :--- | :--- | :--- |
| `start_date` / `end_date` | Date | Filter by range. |
| `year` | Int | Filter by specific year (e.g., 2024). |
| `country_codes` | List[Str] | **OR Logic:** Include views from these countries (e.g., `["US", "ET"]`). |
| `exclude_country_codes` | List[Str] | **NOT Logic:** Exclude views from these countries. |
| `author_username` | Str | Filter by specific author. |

---

### 1. Grouped Analytics
**Endpoint:** `POST /api/analytics/blog-views/{object_type}/`
**Params:** `object_type` = `country` or `user`
**Goal:** Group views by Country, excluding the US.

**Request Body:**
```json
{
  "year": 2024,
  "exclude_country_codes": ["US"]
}
```

**CURL Command:**
```bash
curl -X POST http://localhost:8000/api/analytics/blog-views/country/ \
-H "Content-Type: application/json" \
-d '{ "year": 2024, "exclude_country_codes": ["US"] }'
```

---

### 2. Top Performers
**Endpoint:** `POST /api/analytics/top/{top_type}/`
**Params:** `top_type` = `blog`, `user`, or `country`
**Goal:** Get Top 10 Blogs (No filters).

**Request Body:**
```json
{} 
```

**CURL Command:**
```bash
curl -X POST http://localhost:8000/api/analytics/top/blog/ \
-H "Content-Type: application/json" \
-d '{}'
```

---

### 3. Time-Series Performance
**Endpoint:** `POST /api/analytics/performance/`
**Goal:** Show weekly growth trends (Forcing `week` granularity to meet requirement).

**Request Body:**
```json
{
  "year": 2023,
  
}
```

**CURL Command:**
```bash
curl -X POST http://localhost:8000/api/analytics/performance/ \
-H "Content-Type: application/json" \
-d '{ "year": 2023, "compare_period": "week" }'
```

---





## üèó Architecture & Design Decisions

### 1. Database Optimization (PostgreSQL)
*   **Schema Design:** Used a Star Schema-lite approach. `BlogView` acts as the Fact table.
*   **Indexing:** Added Composite Indexes `(timestamp, country)` and `(blog, timestamp)` to the `BlogView` model. This ensures that filtering by time range and grouping by country remains `O(log n)` rather than a full table scan `O(n)`.
*   **Aggregation:** Leveraged Django ORM's `values()` and `annotate()` to perform `GROUP BY` operations at the database level, preventing N+1 query issues.

### 2. Caching Strategy (Redis)
*   **Problem:** Analytics queries involving `COUNT` and `GROUP BY` on large datasets are expensive.
*   **Solution:** Implemented a Caching Layer in `services.py`.
    *   Cache keys are deterministically generated using an MD5 hash of the sorted filter parameters.
    *   Heavy aggregation results are cached for **15 minutes**.
    *   This reduces API response time from ~200ms (uncached) to <10ms (cached).

### 4. Authentication & Security (JWT)
*   **Context:** The Job Description requested `SimpleJWT`, but the Assessment Requirements focused on public analytics data.
*   **Decision:** I have fully configured `rest_framework_simplejwt`.
    *   JWT Endpoints exist at `/api/token/`.
    *   **Note:** To make the assessment easier to review/test, the Views are currently set to `AllowAny`. In a production environment, I would switch the permission class to `IsAuthenticated`.


### 5. Infrastructure Strategy
*   **Development:** The database is containerized via Docker Compose for rapid setup and reproducibility. Data persistence is handled via Docker Volumes.
*   **Production:** In a real-world deployment, I would **decouple** the database from the Docker cluster. I would utilize a managed service (e.g., **AWS RDS for PostgreSQL** or **Azure Database**) to ensure automated backups, high availability, and independent scaling.
---

## üèó Refactoring & Design Decisions

Based on feedback, I performed a significant refactor to prioritize **Readability, Standardization, and Security**.

### 1. Simplification ("Problem Solver" Approach)
*   **Removed Complexity:** I replaced the custom Recursive Query Builder with a standard, explicit service layer. This significantly improves code readability and maintainability for the team.
*   **Auto-Granularity:** Instead of relying on user input for time-series granularity (which risks performance issues), the system now **auto-calculates** the optimal period (Day/Week/Month) based on the date range.

### 2. Security & Validation
*   **Strict Serialization:** Replaced generic dictionary inputs with a typed `AnalyticsFilterSerializer`. This validates every field (`year`, `country_codes`) and prevents Logic/SQL Injection.
*   **Schema Normalization:** Migrated from raw string storage to a relational `Country` model (Foreign Key) to ensure Data Integrity (3NF).

### 3. Performance
*   **Indexing:** Added Composite Indexes `(timestamp)` and `(blog, timestamp)` to the `BlogView` model.
*   **Redis Caching:** Heavy aggregation results are cached for 15 minutes. Cache keys are deterministically generated based on the filter payload.


## üõ† Tech Stack

*   **Language:** Python 3.11
*   **Framework:** Django 4.2 + Django REST Framework
*   **Database:** PostgreSQL 15
*   **Caching:** Redis 7
*   **Documentation:** drf-yasg (Swagger/OpenAPI)
*   **Containerization:** Docker & Docker Compose

---

## üîÆ Future Improvements & Scaling

If this were a real-world high-traffic system (1M+ requests/day), I would implement the following:

1.  **Asynchronous Processing (Celery):**
    Instead of calculating analytics on-the-fly (Read-Heavy), I would use Celery Beat to pre-calculate daily/weekly stats and store them in a summary table (`AnalyticsReport`).
2.  **OLAP Database:**
    Postgres is great, but for massive analytics, I would sync the `BlogView` data to **ClickHouse** or **Elasticsearch** for sub-second aggregations on billions of rows.
3.  **Partitioning:**
    Partition the `BlogView` table by `timestamp` (Monthly) to keep index sizes manageable.

---

